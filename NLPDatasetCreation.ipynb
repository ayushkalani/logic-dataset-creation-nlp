{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDatasetCreation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOacMIgh2SV7SOLFQaJ1pgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushkalani/logic-dataset-creation-nlp/blob/master/NLPDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8abJ-Askzc",
        "outputId": "d08adbe3-93e2-47ab-9314-739d4d370730"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjY_fLhYa2ao"
      },
      "source": [
        "# Wordnet\n",
        "!pip install names\n",
        "CONCEPT_NET_URL = 'http://api.conceptnet.io/query'\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmT2UxfP7tev"
      },
      "source": [
        "**UTILITY COMMON FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07v3qCX7riu"
      },
      "source": [
        "import names, requests\n",
        "# CONSTANTS\n",
        "PATTERN = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "CSV_COLUMNS = ['UUID','Rules','Context','Questions','Answers']\n",
        "FREQUENT_WORDS = [\"animals\", \"countries\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\", 'area','book','business','case','child','company','country','day','eye','fact','family','government','group','hand','home','job','life','lot','man','money','month','mother','Mr','night','number','part','people','place','point','problem','program','question','right','room','school','state','story','student','study','system','thing','time','water','way','week','woman','word','work','world','year']\n",
        "RELATIONS = ['RelatedTo','IsA','FormOf','PartOf','SimilarTo','HasA','UsedFor','CapableOf','AtLocation','Causes','HasSubevent','HasPrerequisite','HasProperty','MotivatedByGoal','ObstructedBy','Desires','CreatedBy','Synonym','DistinctFrom','DerivedFrom','SymbolOf','DefinedAs','MannerOf','LocatedNear','HasContext','EtymologicallyRelatedTo','EtymologicallyDerivedFrom','CausesDesire','MadeOf','ReceivesAction']\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "# Relations\n",
        "CONJUNCTION_RELATIONS = [\"HasA\"]\n",
        "ADDITION_RELATIONS = [\"HasA\"]\n",
        "COMPOSITION_RELATIONS = [\"HasPrerequisite\", \"MannerOf\"]\n",
        "DEMORGAN_1_RELATIONS = [\"HasA\"]\n",
        "\n",
        "COMPOSITION_WORDS = [\"play\", \"dream\"]\n",
        "\n",
        "# NOUNS = [word for synset in wordnet.all_synsets('n') for word in synset.lemma_names()\n",
        "\n",
        "def create_premise(klass, *sentences):\n",
        "  premise = \"\"\n",
        "  for s in sentences:\n",
        "    premise += (s + \". \")\n",
        "  name = names.get_first_name()\n",
        "  inference = \"{} is a {}\".format(name, klass)\n",
        "  premise += inference\n",
        "  return [premise, name]\n",
        "\n",
        "def question_answer_pairs(questions, ans):\n",
        "  return {\"Questions\": questions, \"Answers\": [ans] * len(questions) }\n",
        "\n",
        "def get_synonym_of_word(klass):\n",
        "  url = '{}?end=/c/en/{}&rel=/r/Synonym&limit=1000'.format(CONCEPT_NET_URL, klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')     \n",
        "\n",
        "def get_antonym_of_word(klass):\n",
        "  url = '{}?end=/c/en/{}&rel=/r/Antonym&limit=1000&filter=/c/en'.format(CONCEPT_NET_URL, klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or getAntonym(klass))\n",
        "\n",
        "def getSynonym(word):\n",
        "  synonyms = []\n",
        "  for syn in wordnet.synsets(word):\n",
        "      for lm in syn.lemmas():\n",
        "              synonyms.append(lm.name())\n",
        "  for syn in synonyms:\n",
        "    if word != syn:\n",
        "      return (syn or '')\n",
        "  return ''\n",
        "\n",
        "def getAntonym(word):\n",
        "  antonyms = []\n",
        "  for syn in wordnet.synsets(word):\n",
        "      for lm in syn.lemmas():\n",
        "          if lm.antonyms():\n",
        "              antonyms.append(lm.antonyms()[0].name())\n",
        "  for syn in antonyms:\n",
        "    if word != syn:\n",
        "      return (syn or '')\n",
        "  return ''\n",
        "\n",
        "def verb_list():\n",
        "  url = 'https://raw.githubusercontent.com/dariusk/corpora/master/data/words/verbs.json'\n",
        "  response = requests.get(url).json()\n",
        "  verb_list = []\n",
        "  for verb in response['verbs']:\n",
        "    verb_list.append(verb['present'])\n",
        "  return verb_list\n",
        "\n",
        "VERB_LIST = verb_list()\n",
        "#adding more words\n",
        "COMPOSITION_WORDS.extend(VERB_LIST)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdoJ4jRn02T"
      },
      "source": [
        "**CONJUNCTION:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKorzDn-fOh"
      },
      "source": [
        "def create_question_type_1(questionable_name, word):\n",
        "  return \"Does {} has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_2(questionable_name, word):\n",
        "  return \"Does {} not has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_3(questionable_name, word_1, word_2, operator):\n",
        "  return \"Does {} has {} {} {}?\".format(questionable_name, word_1, operator, word_2)\n",
        "\n",
        "# does john has tail?\n",
        "def create_question_type_4(questionable_name, word):\n",
        "   antonym = get_antonym_of_word(word)\n",
        "   if antonym:\n",
        "      return \"Does {} has {}?\".format(questionable_name, antonym)\n",
        "   else:\n",
        "     return None"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5wwb71WW-L"
      },
      "source": [
        "# Conjunction\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "\n",
        "def mapping_for_conjunction(klass, rel):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/{}&limit=1000'.format(CONCEPT_NET_URL, klass, rel)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(PATTERN, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          premise, questionable_name = create_premise(klass, sent_1, sent_2)\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []\n",
        "          # does john has tail?\n",
        "          for w in [word_1, word_2]:\n",
        "            questions_type1.append(create_question_type_1(questionable_name, w))\n",
        "            questions_type2.append(create_question_type_2(questionable_name, w))\n",
        "            questions_type4.append(create_question_type_4(questionable_name, w))\n",
        "          # does johhn has tail or fur?\n",
        "          # does johhn has tail and fur?\n",
        "          for oprand in OPERATORS:\n",
        "            questions_type3.append(create_question_type_3(questionable_name, word_1, word_2, oprand))\n",
        "          questions_type4_final_list = list(filter(None, questions_type4))\n",
        "          q1 = question_answer_pairs(questions_type1, YES)\n",
        "          q2 = question_answer_pairs(questions_type2, NO)\n",
        "          q3 = question_answer_pairs(questions_type3, YES)\n",
        "          q4 = question_answer_pairs(questions_type4_final_list, NO)\n",
        "          result['Rules'] = 'Conjunction'\n",
        "          result['Context'] = premise\n",
        "          result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions']\n",
        "          result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers']\n",
        "          df_klass = pd.concat([df_klass, pd.DataFrame(result)], ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HT205vVC5Dd"
      },
      "source": [
        "from uuid import uuid4\n",
        "dataset_df = pd.DataFrame(columns = CSV_COLUMNS)\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\"]\n",
        "\n",
        "for noun in current_words:\n",
        "  for rel in CONJUNCTION_RELATIONS:\n",
        "    try:\n",
        "      dn = mapping_for_conjunction(noun, rel)\n",
        "      dataset_df = pd.concat([dataset_df, pd.DataFrame(dn)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      continue\n",
        "# df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "# len(df)\n",
        "# df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPTriTzANaqX"
      },
      "source": [
        "**ADDITION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsRv-x5eUAQx"
      },
      "source": [
        "import inflect\n",
        "inflect = inflect.engine()\n",
        "\n",
        "def is_word_plural(w):\n",
        "  try:\n",
        "    return not inflect.singular_noun(w) == False\n",
        "  except Exception as e:\n",
        "    return False\n",
        "\n",
        "def create_addition_question_type_1(questionable_name, entity):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/RelatedTo'.format(CONCEPT_NET_URL, entity)\n",
        "  obj = requests.get(url).json()\n",
        "  questions = []\n",
        "\n",
        "  for edge in obj['edges']:\n",
        "    questions.append(\"Is {} a {} or {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "    if (not is_word_plural(entity)) and (not is_word_plural(edge['end']['label'])):\n",
        "      questions.append(\"Is {} a {} or not a {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "  return questions\n",
        "\n",
        "def create_addition_question_type_2(questionable_name, entity):\n",
        "  return \"Is {} not a {}?\".format(questionable_name, entity)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5pSdOrT6Mf8"
      },
      "source": [
        "# Addition\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "\n",
        "def mapping_for_addition(klass, relation):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/{}&limit=1000'.format(CONCEPT_NET_URL, klass, relation)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(PATTERN, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    try:\n",
        "        word_1 = temp[i]\n",
        "        premise, questionable_name = create_premise(klass, dataum[word_1])\n",
        "        questions_type1 = []; questions_type2 = []\n",
        "        questions_type1 = create_addition_question_type_1(questionable_name, word_1)\n",
        "        questions_type2.append(create_addition_question_type_2(questionable_name, word_1))\n",
        "        q1 = question_answer_pairs(questions_type1, YES)\n",
        "        q2 = question_answer_pairs(questions_type2, NO)\n",
        "        result['Rules'] = 'Addition'\n",
        "        result['Context'] = premise\n",
        "        result['Questions'] = q1['Questions'] + q2['Questions']\n",
        "        result['Answers'] = q1['Answers'] + q2['Answers']\n",
        "        df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNxgNphuIv3t"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=CSV_COLUMNS)\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\", 'area','book','business','case','child','company','country','day','eye','fact','family','government','group','hand','home','job','life','lot','man','money','month','mother','Mr','night','number','part','people','place','point','problem','program','question','right','room','school','state','story','student','study','system','thing','time','water','way','week','woman','word','work','world','year']\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_addition(noun, \"IsA\")\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "len(df)\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUQZeTXyoAJa"
      },
      "source": [
        "**COMPOSITION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtVQuJfvoWlK"
      },
      "source": [
        "# If Javier play, will Javier have fun?\n",
        "# If Javier not play, will Javier have fun?\n",
        "def create_composition_question_type_1(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "    questions.append(\"If {} {}, will {} {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "    questions.append(\"If {} not {}, will  {} {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# If Javier play, will Javier have fun and/or learn? YES\n",
        "def create_composition_question_type_2(entity, questional_name, operator, word_1, word_2):\n",
        "  return \"If {} {}, will {} {} {} {}?\".format(questional_name, entity, questional_name, word_1, operator, word_2)\n",
        "\n",
        "# If Javier have fun, will Javier not play? NO\n",
        "def create_composition_question_type_3(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "     questions.append(\"If {} {}, will {} not {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# P = False, Q= False, p ->q TRUE, ANS = yes\n",
        "def create_composition_question_type_4(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "     questions.append(\"If {} not {}, will {} not {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# P = TRue, Q AND R = False, ans = NO\n",
        "def create_composition_question_type_5(entity, questionable_name, word_1, word_2):\n",
        "  return \"If {} {}, will {} not {} and {}?\".format(questionable_name, entity, questionable_name, word_1, word_2)\n",
        "\n",
        "# P = TRue, Q AND R = False, ans = NO\n",
        "def create_composition_question_type_6(entity, questionable_name, word_1, word_2):\n",
        "  return \"If {} not {}, will {} {} and {}?\".format(questionable_name, entity, questionable_name, word_1, word_2)\n",
        "      "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4ZakeMcoW15"
      },
      "source": [
        "# Composition\n",
        "import requests, re, traceback, names\n",
        "import pandas as pd\n",
        "\n",
        "# If you want to [[play]] then you should [[have fun]]. If you want to [[play]] then you should [[relax]]. \n",
        "def mapping_for_composition(klass, relation):\n",
        "  obj = requests.get('{}?start=/c/en/{}&rel=/r/{}'.format(CONCEPT_NET_URL, klass, relation)).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(PATTERN, \"\", edge['surfaceText'])\n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          questionable_name = names.get_first_name()\n",
        "          premise = sent_1 + \". \" + sent_2 + \". \"\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []; questions_type5 = []; questions_type6 = []\n",
        "          questions_type1 = create_composition_question_type_1(klass, questionable_name, word_1, word_2)\n",
        "          for operand in OPERATORS:\n",
        "            questions_type2.append(create_composition_question_type_2(klass, questionable_name, operand, word_1, word_2))\n",
        "          questions_type3 = create_composition_question_type_3(klass, questionable_name, word_1, word_2)\n",
        "          questions_type4 = create_composition_question_type_4(klass, questionable_name, word_1, word_2)\n",
        "          questions_type5.append(create_composition_question_type_5(klass, questionable_name, word_1, word_2))\n",
        "          questions_type6.append(create_composition_question_type_6(klass, questionable_name, word_1, word_2))\n",
        "          q1 = question_answer_pairs(questions_type1, YES)\n",
        "          q2 = question_answer_pairs(questions_type2, YES)\n",
        "          q3 = question_answer_pairs(questions_type3, NO)\n",
        "          q4 = question_answer_pairs(questions_type4, YES)\n",
        "          q5 = question_answer_pairs(questions_type5, NO)\n",
        "          q6 = question_answer_pairs(questions_type6, NO)\n",
        "          result['Rules'] = 'Composition'\n",
        "          result['Context'] = premise\n",
        "          result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions'] + q5['Questions'] + q6['Questions']\n",
        "          result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers'] + q5['Answers'] + q6['Answers']\n",
        "          df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16jEpJk6oovF"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=CSV_COLUMNS)\n",
        "\n",
        "for noun in COMPOSITION_WORDS:\n",
        "  try:\n",
        "    for rel in COMPOSITION_RELATIONS:\n",
        "      dn = mapping_for_composition(noun, rel)\n",
        "      df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzLM_hAQwykr"
      },
      "source": [
        "**De Morgan's Theorem (1)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwlN8U87DK5y"
      },
      "source": [
        "**P AND Q** = Miguel has a cellphone and he has a laptop computer can be written as Miguel has both a cellphone and a laptop compute. <br />\n",
        "***negation, NOT (P AND Q) is (NOT P) OR (NOT Q)***<br />\n",
        "Explaination - \"Miguel does not have both a cellphone and a laptop computer\", which means that he is missing at least one of the two items:\n",
        "he may not have a cellphone, or he may not have a laptop, or he may have neither. Mathematically, if we use \"or\", there's no need to add the \"or both\" at the end, so we can shorten it to this: miguel does not have a cell phone or doels not have a laptop\n",
        " <br />\n",
        "**SOLUTION** Miguel does not have a cellphone or he does not have a laptop computer.‚Äù\n",
        "\n",
        "Reference - [DeMorgan1](https://math.stackexchange.com/questions/2890131/example-of-use-de-morgan-law-and-the-plain-english-behind-it)\n",
        "[DeMorgan Explained](https://stackoverflow.com/questions/2168603/de-morgans-rules-explained)\n",
        "[Examples](https://courses.lumenlearning.com/waymakermath4libarts/chapter/demorgans-laws/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE7m_8HkGYGH"
      },
      "source": [
        "def create_neg_premise1(questional_name, word_1, word_2):\n",
        "  return \"{} does not have both {} and does not have {}\".format(questional_name, word_1, word_2)\n",
        "\n",
        "def create_neg_premise2(questional_name, word_1, word_2):\n",
        "  return \"{} does not have either {} or {}\".format(questional_name, word_1, word_2)\n",
        "\n",
        "# does mike does not have a phone?\n",
        "# does mike does not have a charger?\n",
        "# ans = YES\n",
        "def create_dm_question_type_1(entity, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "    questions.append(\"Does {} does not have {}?\".format(entity, w))\n",
        "  return questions\n",
        "\n",
        "# does mike does have a phone?\n",
        "# does mike does have a charger?\n",
        "# ans = NO\n",
        "def create_dm_question_type_2(entity, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "    questions.append(\"Does {} have {}?\".format(entity, w))\n",
        "  return questions\n",
        "\n",
        "#YES\n",
        "def create_dm_question_type_3(entity, word_1, word_2):\n",
        "  return \"Does {} have {} and not have {}?\".format(entity, word_1, word_2)\n",
        "#YES\n",
        "def create_dm_question_type_4(entity, word_1, word_2):\n",
        "  return \"Does {} not have {} and have {}?\".format(entity, word_1, word_2)  \n",
        "#YES\n",
        "def create_dm_question_type_5(entity, word_1, word_2):\n",
        "  return \"Does {} not have {} and not have {}?\".format(entity, word_1, word_2)\n",
        "#NO\n",
        "def create_dm_question_type_6(entity, word_1, word_2):\n",
        "  return \"Does {} have {} and have {}?\".format(entity, word_1, word_2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9czlJp2B43W"
      },
      "source": [
        "# De Morgan's 1\n",
        "import requests, re, traceback, names\n",
        "import pandas as pd\n",
        "\n",
        "#  Miguel has a [[cellphone]] and he has a [[laptop computer]]\n",
        "# I pay taxes and I vote.\"\n",
        "def mapping_for_demorgan_1(klass, relation):\n",
        "  obj = requests.get('{}?start=/c/en/{}&rel=/r/{}'.format(CONCEPT_NET_URL, klass, relation)).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(PATTERN, \"\", edge['surfaceText'])\n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []; questions_type5 = []; questions_type6 = []\n",
        "          premises = [create_neg_premise1(klass, word_1, word_2), create_neg_premise2(klass, word_1, word_2)]\n",
        "          for premise in premises:\n",
        "            questions_type1.extend(create_dm_question_type_1(klass, word_1, word_2))\n",
        "            questions_type2.extend(create_dm_question_type_2(klass, word_1, word_2))\n",
        "            questions_type3.append(create_dm_question_type_3(klass, word_1, word_2))\n",
        "            questions_type4.append(create_dm_question_type_4(klass, word_1, word_2))\n",
        "            questions_type5.append(create_dm_question_type_5(klass, word_1, word_2))\n",
        "            questions_type5.append(create_dm_question_type_6(klass, word_1, word_2))\n",
        "            \n",
        "            q1 = question_answer_pairs(questions_type1, YES)\n",
        "            q2 = question_answer_pairs(questions_type2, NO)\n",
        "            q3 = question_answer_pairs(questions_type3, YES)\n",
        "            q4 = question_answer_pairs(questions_type4, YES)\n",
        "            q5 = question_answer_pairs(questions_type5, YES)\n",
        "            q6 = question_answer_pairs(questions_type6, NO)\n",
        "\n",
        "            result['Rules'] = 'De Morgan Law 1'\n",
        "            result['Context'] = premise\n",
        "            result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions'] + q5['Questions'] + q6['Questions']\n",
        "            result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers'] + q5['Answers'] + q6['Answers']\n",
        "            df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAhYE-u6NOH"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=CSV_COLUMNS)\n",
        "current_words = [\"animals\", \"countries\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\", 'area','book','business','case','child','company','country','day','eye','fact','family','government','group','hand','home','job','life','lot','man','money','month','mother','Mr','night','number','part','people','place','point','problem','program','question','right','room','school','state','story','student','study','system','thing','time','water','way','week','woman','word','work','world','year']\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_demorgan_1(noun, \"HasA\")\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "len(df)\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZUHJRtaCOod"
      },
      "source": [
        "**Driver Method To generate single CSV dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB2LXIXjCVd7",
        "outputId": "89678133-93e9-4b91-df6d-4e1e966862fd"
      },
      "source": [
        "from uuid import uuid4\n",
        "dataset_df = pd.DataFrame(columns=CSV_COLUMNS)\n",
        "\n",
        "\n",
        "# Conjunction\n",
        "# might timeout run seperately and append\n",
        "for noun in FREQUENT_WORDS:\n",
        "  for rel in CONJUNCTION_RELATIONS:\n",
        "    try:\n",
        "      con_df = mapping_for_conjunction(noun, rel)\n",
        "      dataset_df = pd.concat([dataset_df, pd.DataFrame(con_df)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      continue\n",
        "\n",
        "# Addition\n",
        "for noun in FREQUENT_WORDS:\n",
        "  for rel in ADDITION_RELATIONS:\n",
        "    try:\n",
        "      add_df = mapping_for_addition(noun, rel)\n",
        "      dataset_df = pd.concat([dataset_df, pd.DataFrame(add_df)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      continue\n",
        "\n",
        "# Composition\n",
        "for noun in COMPOSITION_WORDS:\n",
        "  for rel in COMPOSITION_RELATIONS:\n",
        "    try:\n",
        "      comp_df = mapping_for_composition(noun, rel)\n",
        "      dataset_df = pd.concat([dataset_df, pd.DataFrame(comp_df)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      continue\n",
        "\n",
        " # De Morgan 1\n",
        "for noun in FREQUENT_WORDS:\n",
        "  for rel in DEMORGAN_1_RELATIONS:\n",
        "    try:\n",
        "      dm_df = mapping_for_demorgan_1(noun, rel)\n",
        "      dataset_df = pd.concat([dataset_df, pd.DataFrame(dm_df)], ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      continue   \n",
        "\n",
        "dataset_df['UUID'] = dataset_df.index.to_series().map(lambda x: uuid4())\n",
        "dataset_df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)\n",
        "len(dataset_df)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "383141"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBUDdJIo3rBY",
        "outputId": "ce581ddd-207e-4266-e367-2dce03059d9b"
      },
      "source": [
        "conjunction_df = dataset_df\n",
        "len(conjunction_df)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6055"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}