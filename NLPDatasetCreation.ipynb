{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDatasetCreation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwZI6IJzCLhlJQ/gdUj2j8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushkalani/logic-dataset-creation-nlp/blob/master/NLPDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8abJ-Askzc",
        "outputId": "31f46ee1-68b2-46fc-ec8f-82da094a0811"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjY_fLhYa2ao",
        "outputId": "31aac05b-205c-45ef-a5c9-30463c2d7903"
      },
      "source": [
        "# Wordnet\n",
        "!pip install names\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# from nltk.corpus import wordnet as wn\n",
        "\n",
        "# lem = wn.lemmas('frustration')\n",
        "# print(lem)\n",
        "# NOUNS = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: names in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[Lemma('frustration.n.01.frustration'), Lemma('frustration.n.02.frustration'), Lemma('frustration.n.03.frustration')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPecOoJD9ZvZ"
      },
      "source": [
        "relations = ['RelatedTo','IsA','FormOf','PartOf','SimilarTo','HasA','UsedFor','CapableOf','AtLocation','Causes','HasSubevent','HasPrerequisite','HasProperty','MotivatedByGoal','ObstructedBy','Desires','CreatedBy','Synonym','DistinctFrom','DerivedFrom','SymbolOf','DefinedAs','MannerOf','LocatedNear','HasContext','EtymologicallyRelatedTo','EtymologicallyDerivedFrom','CausesDesire','MadeOf','ReceivesAction']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKorzDn-fOh"
      },
      "source": [
        "import names\n",
        "\n",
        "def create_premise(klass, *sentences):\n",
        "  premise = \"\"\n",
        "  for s in sentences:\n",
        "    premise += (s + \". \")\n",
        "  name = names.get_first_name()\n",
        "  inference = \"{} is a {}\".format(name, klass)\n",
        "  #premise += (\". \"+ inference)\n",
        "  premise += inference\n",
        "  return [premise, name]\n",
        "\n",
        "def get_qa(questions, ans):\n",
        "  return {\"Questions\": questions, \"Answers\": [ans] * len(questions) }\n",
        "\n",
        "def create_question_type_1(questionable_name, word):\n",
        "  return \"Does {} has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_2(questionable_name, word):\n",
        "  return \"Does {} not has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_3(questionable_name, word_1, word_2, operator):\n",
        "  return \"Does {} has {} {} {}?\".format(questionable_name, word_1, operator, word_2)\n",
        "\n",
        "# does john has tail?\n",
        "def create_question_type_4(questionable_name, word):\n",
        "   antonym = get_antonym_of_word(word)\n",
        "   if antonym:\n",
        "      return \"Does {} has {}?\".format(questionable_name, antonym)\n",
        "   else:\n",
        "     return None\n",
        "\n",
        "def get_synonym_of_word(klass):\n",
        "  url = 'http://api.conceptnet.io/query?end=/c/en/{}&rel=/r/Synonym&limit=1000'.format(klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')     \n",
        "\n",
        "def get_antonym_of_word(klass):\n",
        "  url = 'http://api.conceptnet.io/query?end=/c/en/{}&rel=/r/Antonym&limit=1000&filter=/c/en'.format(klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5wwb71WW-L"
      },
      "source": [
        "# Conjunction\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "def mapping_for_conjunction(klass):\n",
        "  url = 'http://api.conceptnet.io/query?start=/c/en/{}&rel=/r/HasA&limit=1000'.format(klass)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          premise, questionable_name = create_premise(klass, sent_1, sent_2)\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []\n",
        "          # does john has tail?\n",
        "          for w in [word_1, word_2]:\n",
        "            questions_type1.append(create_question_type_1(questionable_name, w))\n",
        "            questions_type2.append(create_question_type_2(questionable_name, w))\n",
        "            questions_type4.append(create_question_type_4(questionable_name, w))\n",
        "          # does johhn has tail or fur?\n",
        "          # does johhn has tail and fur?\n",
        "          for oprand in OPERATORS:\n",
        "            questions_type3.append(create_question_type_3(questionable_name, word_1, word_2, oprand))\n",
        "          questions_type4_final_list = list(filter(None, questions_type4))\n",
        "          q1=get_qa(questions_type1, YES)\n",
        "          q2=get_qa(questions_type2, NO)\n",
        "          q3=get_qa(questions_type3, YES)\n",
        "          q4=get_qa(questions_type4_final_list, NO)\n",
        "          result['Rules'] = 'Conjunction'\n",
        "          result['Context'] = premise\n",
        "          result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions']\n",
        "          result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers']\n",
        "          df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HT205vVC5Dd"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\"]\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_conjunction(noun)\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyHP2KW49FOS"
      },
      "source": [
        "len(df)\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPTriTzANaqX"
      },
      "source": [
        "### ADdDITON\n",
        "**ADDITIOn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsRv-x5eUAQx"
      },
      "source": [
        "import inflect\n",
        "inflect = inflect.engine()\n",
        "\n",
        "def is_word_plural(w):\n",
        "  try:\n",
        "    return not inflect.singular_noun(w) == False\n",
        "  except Exception as e:\n",
        "    return False\n",
        "\n",
        "def create_addition_question_type_1(questionable_name, entity):\n",
        "  url = 'https://api.conceptnet.io/query?start=/c/en/{}&rel=/r/RelatedTo'.format(entity)\n",
        "  obj = requests.get(url).json()\n",
        "  questions = []\n",
        "\n",
        "  for edge in obj['edges']:\n",
        "    questions.append(\"Is {} a {} or {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "    if (not is_word_plural(entity)) and (not is_word_plural(edge['end']['label'])):\n",
        "      questions.append(\"Is {} a {} or not a {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "  return questions\n",
        "\n",
        "def create_addition_question_type_2(questionable_name, entity):\n",
        "  return \"Is {} not a {}?\".format(questionable_name, entity)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5pSdOrT6Mf8"
      },
      "source": [
        "# Addition\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "def mapping_for_addition(klass, relation):\n",
        "  url = 'http://api.conceptnet.io/query?start=/c/en/{}&rel=/r/{}&limit=1000'.format(klass, relation)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    try:\n",
        "        word_1 = temp[i]\n",
        "        premise, questionable_name = create_premise(klass, dataum[word_1])\n",
        "        questions_type1 = []; questions_type2 = []\n",
        "        questions_type1 = create_addition_question_type_1(questionable_name, word_1)\n",
        "        questions_type2.append(create_addition_question_type_2(questionable_name, word_1))\n",
        "        q1=get_qa(questions_type1, YES)\n",
        "        q2=get_qa(questions_type2, NO)\n",
        "        result['Rules'] = 'Addition'\n",
        "        result['Context'] = premise\n",
        "        result['Questions'] = q1['Questions'] + q2['Questions']\n",
        "        result['Answers'] = q1['Answers'] + q2['Answers']\n",
        "        df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNxgNphuIv3t"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\"]\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_addition(noun, \"IsA\")\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 194,
      "outputs": []
    }
  ]
}