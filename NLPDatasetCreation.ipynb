{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDatasetCreation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9aT3W3VYY0rLNnThrtu/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushkalani/logic-dataset-creation-nlp/blob/master/NLPDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8abJ-Askzc",
        "outputId": "31f46ee1-68b2-46fc-ec8f-82da094a0811"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjY_fLhYa2ao",
        "outputId": "59ea3a48-1a5f-4171-ce5d-0f234e4cd55c"
      },
      "source": [
        "# Wordnet\n",
        "!pip install names\n",
        "CONCEPT_NET_URL = 'http://api.conceptnet.io/query'\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# from nltk.corpus import wordnet as wn\n",
        "\n",
        "# lem = wn.lemmas('frustration')\n",
        "# print(lem)\n",
        "# NOUNS = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]\n",
        "#relations = ['RelatedTo','IsA','FormOf','PartOf','SimilarTo','HasA','UsedFor','CapableOf','AtLocation','Causes','HasSubevent','HasPrerequisite','HasProperty','MotivatedByGoal','ObstructedBy','Desires','CreatedBy','Synonym','DistinctFrom','DerivedFrom','SymbolOf','DefinedAs','MannerOf','LocatedNear','HasContext','EtymologicallyRelatedTo','EtymologicallyDerivedFrom','CausesDesire','MadeOf','ReceivesAction']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: names in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCdoJ4jRn02T"
      },
      "source": [
        "**CONJUNCTION:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKorzDn-fOh"
      },
      "source": [
        "import names\n",
        "\n",
        "def create_premise(klass, *sentences):\n",
        "  premise = \"\"\n",
        "  for s in sentences:\n",
        "    premise += (s + \". \")\n",
        "  name = names.get_first_name()\n",
        "  inference = \"{} is a {}\".format(name, klass)\n",
        "  #premise += (\". \"+ inference)\n",
        "  premise += inference\n",
        "  return [premise, name]\n",
        "\n",
        "def get_qa(questions, ans):\n",
        "  return {\"Questions\": questions, \"Answers\": [ans] * len(questions) }\n",
        "\n",
        "def create_question_type_1(questionable_name, word):\n",
        "  return \"Does {} has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_2(questionable_name, word):\n",
        "  return \"Does {} not has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_3(questionable_name, word_1, word_2, operator):\n",
        "  return \"Does {} has {} {} {}?\".format(questionable_name, word_1, operator, word_2)\n",
        "\n",
        "# does john has tail?\n",
        "def create_question_type_4(questionable_name, word):\n",
        "   antonym = get_antonym_of_word(word)\n",
        "   if antonym:\n",
        "      return \"Does {} has {}?\".format(questionable_name, antonym)\n",
        "   else:\n",
        "     return None\n",
        "\n",
        "def get_synonym_of_word(klass):\n",
        "  url = '{}?end=/c/en/{}&rel=/r/Synonym&limit=1000'.format(CONCEPT_NET_URL, klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')     \n",
        "\n",
        "def get_antonym_of_word(klass):\n",
        "  url = '{}?end=/c/en/{}&rel=/r/Antonym&limit=1000&filter=/c/en'.format(CONCEPT_NET_URL, klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5wwb71WW-L"
      },
      "source": [
        "# Conjunction\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "def mapping_for_conjunction(klass):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/HasA&limit=1000'.format(CONCEPT_NET_URL, klass)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          premise, questionable_name = create_premise(klass, sent_1, sent_2)\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []\n",
        "          # does john has tail?\n",
        "          for w in [word_1, word_2]:\n",
        "            questions_type1.append(create_question_type_1(questionable_name, w))\n",
        "            questions_type2.append(create_question_type_2(questionable_name, w))\n",
        "            questions_type4.append(create_question_type_4(questionable_name, w))\n",
        "          # does johhn has tail or fur?\n",
        "          # does johhn has tail and fur?\n",
        "          for oprand in OPERATORS:\n",
        "            questions_type3.append(create_question_type_3(questionable_name, word_1, word_2, oprand))\n",
        "          questions_type4_final_list = list(filter(None, questions_type4))\n",
        "          q1=get_qa(questions_type1, YES)\n",
        "          q2=get_qa(questions_type2, NO)\n",
        "          q3=get_qa(questions_type3, YES)\n",
        "          q4=get_qa(questions_type4_final_list, NO)\n",
        "          result['Rules'] = 'Conjunction'\n",
        "          result['Context'] = premise\n",
        "          result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions']\n",
        "          result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers']\n",
        "          df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HT205vVC5Dd"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\"]\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_conjunction(noun)\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "len(df)\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPTriTzANaqX"
      },
      "source": [
        "**ADDITION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsRv-x5eUAQx"
      },
      "source": [
        "import inflect\n",
        "inflect = inflect.engine()\n",
        "\n",
        "def is_word_plural(w):\n",
        "  try:\n",
        "    return not inflect.singular_noun(w) == False\n",
        "  except Exception as e:\n",
        "    return False\n",
        "\n",
        "def create_addition_question_type_1(questionable_name, entity):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/RelatedTo'.format(CONCEPT_NET_URL, entity)\n",
        "  obj = requests.get(url).json()\n",
        "  questions = []\n",
        "\n",
        "  for edge in obj['edges']:\n",
        "    questions.append(\"Is {} a {} or {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "    if (not is_word_plural(entity)) and (not is_word_plural(edge['end']['label'])):\n",
        "      questions.append(\"Is {} a {} or not a {}\".format(questionable_name, entity, edge['end']['label']))\n",
        "  return questions\n",
        "\n",
        "def create_addition_question_type_2(questionable_name, entity):\n",
        "  return \"Is {} not a {}?\".format(questionable_name, entity)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5pSdOrT6Mf8"
      },
      "source": [
        "# Addition\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "def mapping_for_addition(klass, relation):\n",
        "  url = '{}?start=/c/en/{}&rel=/r/{}&limit=1000'.format(CONCEPT_NET_URL, klass, relation)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    try:\n",
        "        word_1 = temp[i]\n",
        "        premise, questionable_name = create_premise(klass, dataum[word_1])\n",
        "        questions_type1 = []; questions_type2 = []\n",
        "        questions_type1 = create_addition_question_type_1(questionable_name, word_1)\n",
        "        questions_type2.append(create_addition_question_type_2(questionable_name, word_1))\n",
        "        q1=get_qa(questions_type1, YES)\n",
        "        q2=get_qa(questions_type2, NO)\n",
        "        result['Rules'] = 'Addition'\n",
        "        result['Context'] = premise\n",
        "        result['Questions'] = q1['Questions'] + q2['Questions']\n",
        "        result['Answers'] = q1['Answers'] + q2['Answers']\n",
        "        df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(traceback.format_exc())\n",
        "      return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNxgNphuIv3t"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"countries\", \"animals\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"dishes\", 'area','book','business','case','child','company','country','day','eye','fact','family','government','group','hand','home','job','life','lot','man','money','month','mother','Mr','night','number','part','people','place','point','problem','program','question','right','room','school','state','story','student','study','system','thing','time','water','way','week','woman','word','work','world','year']\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_addition(noun, \"IsA\")\n",
        "    print(dn)\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "len(df)\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUQZeTXyoAJa"
      },
      "source": [
        "**COMPOSITION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtVQuJfvoWlK"
      },
      "source": [
        "# If Javier play, will Javier have fun?\n",
        "# If Javier not play, will Javier have fun?\n",
        "def create_composition_question_type_1(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "    questions.append(\"If {} {}, will {} {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "    questions.append(\"If {} not {}, will  {} {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# If Javier play, will Javier have fun and/or learn? YES\n",
        "def create_composition_question_type_2(entity, questional_name, operator, word_1, word_2):\n",
        "  return \"If {} {}, will {} {} {} {}?\".format(questional_name, entity, questional_name, word_1, operator, word_2)\n",
        "\n",
        "# If Javier have fun, will Javier not play? NO\n",
        "def create_composition_question_type_3(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "     questions.append(\"If {} {}, will {} not {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# P = False, Q= False, p ->q TRUE, ANS = yes\n",
        "def create_composition_question_type_4(entity, questionable_name, *words):\n",
        "  questions = []\n",
        "  for w in words:\n",
        "     questions.append(\"If {} not {}, will {} not {}?\".format(questionable_name, entity, questionable_name, w))\n",
        "  return questions\n",
        "\n",
        "# P = TRue, Q AND R = False, ans = NO\n",
        "def create_composition_question_type_5(entity, questionable_name, word_1, word_2):\n",
        "  return \"If {} {}, will {} not {} and {}?\".format(questionable_name, entity, questionable_name, word_1, word_2)\n",
        "\n",
        "# P = TRue, Q AND R = False, ans = NO\n",
        "def create_composition_question_type_6(entity, questionable_name, word_1, word_2):\n",
        "  return \"If {} not {}, will {} {} and {}?\".format(questionable_name, entity, questionable_name, word_1, word_2)\n",
        "      "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4ZakeMcoW15"
      },
      "source": [
        "# Composition\n",
        "import requests, re, traceback, names\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "# If you want to [[play]] then you should [[have fun]]. If you want to [[play]] then you should [[relax]]. \n",
        "def mapping_for_composition(klass, relation):\n",
        "  obj = requests.get('{}?start=/c/en/{}&rel=/r/{}'.format(CONCEPT_NET_URL, klass, relation)).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    if edge['surfaceText']:\n",
        "      dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  temp = list(dataum)\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          questionable_name = names.get_first_name()\n",
        "          premise = sent_1 + \". \" + sent_2 + \". \"\n",
        "          questions_type1 = []; questions_type2 = []; questions_type3 = []; questions_type4 = []; questions_type5 = []; questions_type6 = []\n",
        "          questions_type1 = create_composition_question_type_1(klass, questionable_name, word_1, word_2)\n",
        "          for operand in OPERATORS:\n",
        "            questions_type2.append(create_composition_question_type_2(klass, questionable_name, operand, word_1, word_2))\n",
        "          questions_type3 = create_composition_question_type_3(klass, questionable_name, word_1, word_2)\n",
        "          questions_type4 = create_composition_question_type_4(klass, questionable_name, word_1, word_2)\n",
        "          questions_type5.append(create_composition_question_type_5(klass, questionable_name, word_1, word_2))\n",
        "          questions_type6.append(create_composition_question_type_6(klass, questionable_name, word_1, word_2))\n",
        "          q1 = get_qa(questions_type1, YES)\n",
        "          q2 = get_qa(questions_type2, YES)\n",
        "          q3 = get_qa(questions_type3, NO)\n",
        "          q4 = get_qa(questions_type4, YES)\n",
        "          q5 = get_qa(questions_type5, NO)\n",
        "          q6 = get_qa(questions_type6, NO)\n",
        "          result['Rules'] = 'Composition'\n",
        "          result['Context'] = premise\n",
        "          result['Questions'] = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions'] + q5['Questions'] + q6['Questions']\n",
        "          result['Answers'] = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers'] + q5['Answers'] + q6['Answers']\n",
        "          df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16jEpJk6oovF"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"play\", \"dream\"]\n",
        "COMPOSITION_RELATIONS = [\"HasPrerequisite\"] # \"MannerOf\"\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    for rel in COMPOSITION_RELATIONS:\n",
        "      dn = mapping_for_composition(noun, rel)\n",
        "      df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(traceback.format_exc())\n",
        "    continue\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": 80,
      "outputs": []
    }
  ]
}