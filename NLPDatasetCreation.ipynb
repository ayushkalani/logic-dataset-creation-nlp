{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDatasetCreation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5np/RlvfENc0bqCIVI1Wv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushkalani/logic-dataset-creation-nlp/blob/master/NLPDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8abJ-Askzc",
        "outputId": "077426c9-6aaa-4c41-990f-1346dd93b1a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjY_fLhYa2ao",
        "outputId": "fe4451ae-8d23-4af0-9514-4ccf25ee221f"
      },
      "source": [
        "# Wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "NOUNS = [word for synset in wn.all_synsets('n') for word in synset.lemma_names()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZnO74yBCRO5",
        "outputId": "515f0afc-adb9-433e-bb8c-b73bb8749625"
      },
      "source": [
        "!pip install names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: names in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPecOoJD9ZvZ"
      },
      "source": [
        "relations = ['RelatedTo','IsA','FormOf','PartOf','SimilarTo','HasA','UsedFor','CapableOf','AtLocation','Causes','HasSubevent','HasPrerequisite','HasProperty','MotivatedByGoal','ObstructedBy','Desires','CreatedBy','Synonym','DistinctFrom','DerivedFrom','SymbolOf','DefinedAs','MannerOf','LocatedNear','HasContext','EtymologicallyRelatedTo','EtymologicallyDerivedFrom','CausesDesire','MadeOf','ReceivesAction']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKorzDn-fOh"
      },
      "source": [
        "import names\n",
        "\n",
        "def create_premise(klass, sent_1, sent_2):\n",
        "  premise = \"\"\n",
        "  premise += (sent_1 + \". \" + sent_2)\n",
        "  name = names.get_first_name()\n",
        "  inference = \"{} is a {}\".format(name, klass)\n",
        "  premise += (\". \"+ inference)\n",
        "  return [premise, name]\n",
        "\n",
        "def get_qa(questions, ans):\n",
        "  return {\"Questions\": questions, \"Answers\": [ans] * len(questions) }\n",
        "\n",
        "def create_question_type_1(questionable_name, word):\n",
        "  return \"Does {} has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_2(questionable_name, word):\n",
        "  return \"Does {} not has {}?\".format(questionable_name, word)\n",
        "\n",
        "def create_question_type_3(questionable_name, word_1, word_2, operator):\n",
        "  return \"Does {} has {} {} {}?\".format(questionable_name, word_1, operator, word_2)\n",
        "\n",
        "# does john has tail?\n",
        "def create_question_type_4(questionable_name, word):\n",
        "   antonym = get_antonym_of_word(word)\n",
        "   if antonym:\n",
        "      return \"Does {} has {}?\".format(questionable_name, antonym)\n",
        "   else:\n",
        "     return None\n",
        "\n",
        "def get_antonym_of_word(klass):\n",
        "  url = 'http://api.conceptnet.io/query?end=/c/en/{}&rel=/r/Antonym&limit=1000&filter=/c/en'.format(klass)\n",
        "  obj = requests.get(url).json()\n",
        "  # return the first antonym\n",
        "  for edge in obj['edges']:\n",
        "    return (edge['start']['label'] or '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5wwb71WW-L"
      },
      "source": [
        "# Conjunction\n",
        "import requests, re, traceback\n",
        "import pandas as pd\n",
        "pattern = r\"[\\([{})\\]]\"\n",
        "OPERATORS = [\"and\", \"or\"]\n",
        "YES =\"yes\"\n",
        "NO = \"no\"\n",
        "\n",
        "def mapping_for_class(klass):\n",
        "  url = 'http://api.conceptnet.io/query?start=/c/en/{}&rel=/r/HasA&limit=1000'.format(klass)\n",
        "  obj = requests.get(url).json()\n",
        "  dataum = {}\n",
        "  df_klass = pd.DataFrame()\n",
        "  for edge in obj['edges']:\n",
        "    dataum[edge['end']['label']] = re.sub(pattern, \"\", edge['surfaceText'])\n",
        "  \n",
        "  temp = list(dataum)\n",
        "  klass_examples = {}\n",
        "  result = {'Questions': [], 'Answers': []}\n",
        "  for i in range(0, len(temp)):\n",
        "    word_1 = temp[i]\n",
        "    for j in range(i+1, len(temp)):\n",
        "      try:\n",
        "          #word_2 = temp[temp.index(word_1) + 1]\n",
        "          word_2 = temp[j]\n",
        "          sent_1 = dataum[word_1]\n",
        "          sent_2 = dataum[word_2]\n",
        "          premise, questionable_name = create_premise(klass, sent_1, sent_2)\n",
        "          questions_type1 = []\n",
        "          questions_type2 = []\n",
        "          questions_type3 = []\n",
        "          questions_type4 = []\n",
        "          # does john has tail?\n",
        "          for w in [word_1, word_2]:\n",
        "            questions_type1.append(create_question_type_1(questionable_name, w))\n",
        "            questions_type2.append(create_question_type_2(questionable_name, w))\n",
        "            questions_type4.append(create_question_type_4(questionable_name, w))\n",
        "          # does johhn has tail or fur?\n",
        "          # does johhn has tail and fur?\n",
        "          for oprand in OPERATORS:\n",
        "            questions_type2.append(create_question_type_3(questionable_name, word_1, word_2, oprand))\n",
        "          questions_type4_final_list = list(filter(None, questions_type4))\n",
        "          q1=get_qa(questions_type1, YES)\n",
        "          q2=get_qa(questions_type2, NO)\n",
        "          q3=get_qa(questions_type3, YES)\n",
        "          q4=get_qa(questions_type4_final_list, NO)\n",
        "          result['Rules'] = 'Conjunction'\n",
        "          result['Context'] = premise\n",
        "          a = q1['Questions'] + q2['Questions'] + q3['Questions'] + q4['Questions']\n",
        "          result['Questions'].extend(a)\n",
        "          b = q1['Answers'] + q2['Answers'] + q3['Answers'] + q4['Answers']\n",
        "          result['Answers'].extend(b)\n",
        "          df_klass = pd.concat([df_klass,pd.DataFrame(result)],ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "      except (ValueError, IndexError):\n",
        "        print(traceback.format_exc())\n",
        "        return df_klass\n",
        "  return df_klass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLJb3Zh6q57Q"
      },
      "source": [
        "mapping_for_class(\"animals\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HT205vVC5Dd"
      },
      "source": [
        "from uuid import uuid4\n",
        "df = pd.DataFrame(columns=['UUID','Rules','Context','Questions','Answers'])\n",
        "current_words = [\"animals\", \"countries\", \"cities\", \"sports\", \"cultures\", \"dogs\", \"cats\", \"buildings\", \"colleges\", \"subjects\", \"lakes\", \"mountains\", \"social\", \"dishes\"]\n",
        "\n",
        "for noun in current_words:\n",
        "  try:\n",
        "    dn = mapping_for_class(noun)\n",
        "    df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "  except Exception:\n",
        "    continue\n",
        "    #print(traceback.format_exc())\n",
        "\n",
        "# for noun in NOUNS[100:200]:\n",
        "#   dn = mapping_for_class(noun)\n",
        "#   df = pd.concat([df, pd.DataFrame(dn)], ignore_index=True)\n",
        "\n",
        "df['UUID'] = df.index.to_series().map(lambda x: uuid4())\n",
        "df.to_csv(\"Dataset5_AyushKalani.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "4CaAvtID1MTi",
        "outputId": "16e890c5-ffb8-4568-9731-94d30525737f"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e112543b788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}